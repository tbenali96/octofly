{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from catboost import CatBoostClassifier, Pool, CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_parquet(\"../data/processed/train_data/train.gzip\")\n",
    "y = pd.read_parquet(\"../data/processed/train_data/train_target.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in X.columns:\n",
    "    if X[column].isnull().values.any():\n",
    "        print(column, X[column].isnull().sum(), X[column].isnull().sum()/X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_retard = y[[\"RETARD A L'ARRIVEE\"]]\n",
    "y = y[[\"RETARD\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [0,1,6,10,12,13,14,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_neg = y_train[y_train[\"RETARD\"]==0].shape[0]\n",
    "sum_pos = y_train[y_train[\"RETARD\"]==1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations=500, \n",
    "                           learning_rate=0.03, \n",
    "                           eval_metric=\"Recall\",\n",
    "                           depth=10,\n",
    "                           random_seed=0, \n",
    "                           auto_class_weights=\"Balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, cat_features)\n",
    "preds_class = model.predict(X_test)\n",
    "preds_proba = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, preds_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, preds_class)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.001, 1])\n",
    "plt.ylim([0, 1.001])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[X_test.columns] = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in range(preds_proba.shape[0]):\n",
    "    predictions.append(preds_proba[i][1])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"RETARD PREDIT\"] = predictions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.2\n",
    "def calculate_prediction(x, thresh):\n",
    "    if x <= thresh:\n",
    "        return 0\n",
    "    else : \n",
    "        return 1\n",
    "df[\"RETARD\"] = df[\"RETARD PREDIT\"].apply(lambda x: calculate_prediction(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, df[\"RETARD\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERPRETATION WITH SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "shap_values = shap.TreeExplainer(model).shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"NOMBRE DE PASSAGERS\", shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"TEMPS PROGRAMME\", shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"DISTANCE\", shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"HEURE D'ARRIVEE\", shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle de régression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_retard = y_retard[y_retard[\"RETARD A L'ARRIVEE\"]>10]\n",
    "y_retard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = y_retard.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_deleted = X.index.difference(indexes)\n",
    "indexes_deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_retard = X.drop(indexes_deleted)\n",
    "X_retard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_retard = X_retard.reset_index(drop=True)\n",
    "y_retard = y_retard.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [0,1,6,10,12,13,14,15]\n",
    "\n",
    "for i in cat_features:\n",
    "    X_retard.iloc[:,i] = X_retard.iloc[:,i].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_retard, X_test_retard, y_train_retard, y_test_retard = train_test_split(X_retard,\n",
    "                                                                                y_retard, \n",
    "                                                                                test_size=0.2,\n",
    "                                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMRegressor(num_leaves=50, max_depth=-1, \n",
    "                         random_state=314, \n",
    "                         silent=True, \n",
    "                         metric='None', \n",
    "                         n_jobs=4, \n",
    "                         n_estimators=2000,\n",
    "                         colsample_bytree=0.9,\n",
    "                         subsample=0.9,\n",
    "                         learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_retard, y_train_retard)\n",
    "preds_class = clf.predict(X_test_retard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test_retard, preds_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test_retard, preds_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retard = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retard[\"RETARD\"] = y_test_retard[\"RETARD A L'ARRIVEE\"]\n",
    "df_retard[\"RETARD PREDIT\"] = preds_class\n",
    "df_retard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Pool(data=X_train, cat_features=cat_features)\n",
    "pd.DataFrame({'feature_importance': model.get_feature_importance(data), \n",
    "              'feature_names': X_test.columns}).sort_values(by=['feature_importance'], \n",
    "                                                           ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvaltest(params,train_set,train_label,cat_dims,n_splits=3):\n",
    "    kf = KFold(n_splits=n_splits,shuffle=True) \n",
    "    res = []\n",
    "    for train_index, test_index in kf.split(train_set):\n",
    "        train = train_set.iloc[train_index,:]\n",
    "        test = train_set.iloc[test_index,:]\n",
    "\n",
    "        labels = train_label.iloc[train_index]\n",
    "        test_labels = train_label.iloc[test_index]\n",
    "\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(train, np.ravel(labels), cat_features=cat_dims)\n",
    "\n",
    "        #res.append(np.mean(clf.predict(test)==np.ravel(test_labels)))\n",
    "        res.append(recall(test_labels, model.predict(test)))\n",
    "    return np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params ={'learning_rate': 0.03,\n",
    "        'depth': 10,\n",
    "        'iterations': 1000, \n",
    "        'random_seed': 0, \n",
    "        'auto_class_weights': \"Balanced\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvaltest(params, X, y, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': [0.03, 0.05, 0.1],\n",
    "        'depth': [5, 10, 20],\n",
    "        'iterations': [50, 100, 200, 1000], \n",
    "        'random_seed': [0], \n",
    "        'auto_class_weights': [\"Balanced\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb.plot_importance(clf, height=0.5, importance_type='gain', \n",
    "                    max_num_features=15, title= \"Featur Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shap interpretation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons un diagramme de dispersion de densité des valeurs SHAP pour chaque feature afin d'identifier l'impact de chaque feature sur la sortie du modèle pour les individus de l'ensemble de données de test. Les features sont triées en fonction de la somme des valeurs SHAP pour tous les échantillons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clf.booster_)\n",
    "shap_values = explainer.shap_values(X_test_retard)\n",
    "global_importances = np.abs(shap_values).mean(0)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.argsort(-global_importances)\n",
    "f = plt.figure(figsize=(5,10))\n",
    "nb_features = len(global_importances)\n",
    "y_pos = np.arange(nb_features)\n",
    "inds2 = np.flip(inds[:nb_features], 0)\n",
    "plt.barh(y_pos, global_importances[inds2], align='center', color=\"#1E88E5\")\n",
    "plt.yticks(y_pos, fontsize=13)\n",
    "plt.gca().set_yticklabels(X_train.columns[inds2])\n",
    "plt.xlabel('mean abs. SHAP value (impact on model output)', fontsize=13)\n",
    "plt.gca().xaxis.set_ticks_position('bottom')\n",
    "plt.gca().yaxis.set_ticks_position('none')\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test_retard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_retard.iloc[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first prediction's explanation\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], X_test_retard.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize many prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[1], shap_values[:1000,:], X_display.iloc[:1000,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the SHAP dependence plots for the top 5 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les graphiques de dépendance SHAP montrent l'effet d'une seule feature sur l'ensemble des données. Ils représentent la valeur d'une feature en fonction de la valeur SHAP de cette feature sur de nombreux échantillons. Les diagrammes de dépendance SHAP sont similaires aux diagrammes de dépendance partielle, mais tiennent compte des effets d'interaction présents dans les features, et ne sont définis que dans les régions de l'espace d'entrée supportées par les données. La dispersion verticale des valeurs SHAP pour une seule valeur de feature est due aux effets d'interaction, et une autre feature est choisie pour être colorée afin de mettre en évidence les interactions possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in X_test_retard.columns:\n",
    "    shap.dependence_plot(name, shap_values, X_test_retard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
